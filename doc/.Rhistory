textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table()[,1])) %>%
gather(date,z,-x,-y,-land) %>%
mutate(time = as.numeric(date)) %>%
select(-date) %>%
subset(x > 155 & x < 280 & y <14&land==0)
sst <- getURL("http://www.stat.missouri.edu/~wikle/SSTlonlat.dat") %>%
textConnection() %>%
read.table(col.names = c("x","y")) %>%
cbind(getURL("http://www.stat.missouri.edu/~wikle/SST011970_032003.dat") %>%
textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table())[,1])
sst <- getURL("http://www.stat.missouri.edu/~wikle/SSTlonlat.dat") %>%
textConnection() %>%
read.table(col.names = c("x","y")) %>%
cbind(getURL("http://www.stat.missouri.edu/~wikle/SST011970_032003.dat") %>%
textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table())[,1]) %>%
gather(date,z,-x,-y,-land) %>%
mutate(time = as.numeric(date)) %>%
select(-date) %>%
subset(x > 155 & x < 280 & y <14&land==0)
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(EFDR)
library(doMC)
library(foreach)
library(RCurl)
sst <- getURL("http://www.stat.missouri.edu/~wikle/SSTlonlat.dat") %>%
textConnection() %>%
read.table(col.names = c("x","y")) %>%
cbind(getURL("http://www.stat.missouri.edu/~wikle/SST011970_032003.dat") %>%
textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table())[,1]) %>%
gather(date,z,-x,-y,-land) %>%
mutate(time = as.numeric(date)) %>%
select(-date) %>%
subset(x > 155 & x < 280 & y <14&land==0)
head(sst)
tail(sst)
q()
q()
X <- data.frame(x = c(1,1,1,2,2,2,3,4),y=rnorm(8))
library(dplyr)
g <- group_by(X,x)
xxx <- summarise(g,mean(y))
xxx
id <- c(1, 2, 3, 4, 5)
name <- c("name1", "name2", "name3", "name4", "name5")
position <- c("AAA", "BBB", "CCC", "AAA", "BBB")
salary <- c(20, 30, 40, 50, 60)
bonus <- c(1, 1, 1, NA, 1)
sti <- c(2, 3, 4, 5, 6)
lti <- c(6, 5, 4, 3, 2)
other <- c(10, 11, 12, 13, 14)
df <- data.frame(id, name, position, salary, bonus, sti, lti, other)
df_out <- ddply(df, .(position), numcolwise(summary))
g <- group_by(df,position)
summarise(g,summary)
summarise(g,summary(bonus))
df
summarise(g,summary(lti))
summarise(g, mean(lti))
summarise(g, numcolwise(summary)(lti))
summarise(g, t(summary(lti)))
summary(c(1,2,3,4))
summarise(g, stats = summary(lti))
summarise(g, stats = summary(lti)[1])
library(dplyr)
id <- c(1, 2, 3, 4, 5)
name <- c("name1", "name2", "name3", "name4", "name5")
position <- c("AAA", "BBB", "CCC", "AAA", "BBB")
salary <- c(20, 30, 40, 50, 60)
bonus <- c(1, 1, 1, NA, 1)
sti <- c(2, 3, 4, 5, 6)
lti <- c(6, 5, 4, 3, 2)
other <- c(10, 11, 12, 13, 14)
df <- data.frame(id, name, position, salary, bonus, sti, lti, other)
df_out <- ddply(df, .(position), numcolwise(summary))
library(plyr)
q()
library(plyr)
library(EFDR)
vignette()
vignette("EFDR")
vignette("EFDR_SST")
q()
library(maps)
library(mapdata)
library(ggplot2)
library(ggmap)
#pick out the base layer I want
get.Peru<-get_map(location = c(left = -85, bottom = -20, right = -70, top = -5), maptype="satellite")
#this is the layer i wish to put over the top
coast_map <- fortify(map("worldHires", fill = TRUE, plot = FALSE))
peru_bathy_map <- fortify(get.Peru)
gg <- ggplot()
gg <- gg + geom_map(data=peru_bathy_map, map=peru_bathy_map,
aes(map_id=id))
gg <- gg + geom_map(data=coast_map, map=coast_map, aes(x=long, y=lat, map_id=region),
fill="gray", color="black") + xlim(-86,-70) + ylim(-20,-4) + labs(x="Longitude", y="Latitude")
gg <- gg + coord_map() +  theme_classic()
install.packages("mapdata")
install.packages("ggmap")
library(maps)
library(mapdata)
library(ggplot2)
library(ggmap)
#pick out the base layer I want
get.Peru<-get_map(location = c(left = -85, bottom = -20, right = -70, top = -5), maptype="satellite")
#this is the layer i wish to put over the top
coast_map <- fortify(map("worldHires", fill = TRUE, plot = FALSE))
peru_bathy_map <- fortify(get.Peru)
gg <- ggplot()
gg <- gg + geom_map(data=peru_bathy_map, map=peru_bathy_map,
aes(map_id=id))
gg <- gg + geom_map(data=coast_map, map=coast_map, aes(x=long, y=lat, map_id=region),
fill="gray", color="black") + xlim(-86,-70) + ylim(-20,-4) + labs(x="Longitude", y="Latitude")
gg <- gg + coord_map() +  theme_classic()
XX <- get.Peru
class(XX)
XX$ggmap
XX
plot(XX)
image(XX)
head(XX)
dim(XX)
image(XX)
XXX <- as.numeric(XX)
image(XXX)
dim(XXX)
XX[1:10,1:10]
image(XX[1:10,1:10])
as.numeric(XX[1:10,1:10])
strsplit(XX[1:10,1:10],"#")
as.numeric(XX[1:10,1:10])
help(strsplit)
as.numeric(XX[1,1],"#")
strsplit(XX[1,1],"#")
strsplit(XX[1,1],"#")[2]
strsplit(XX[1,1],"#")[[1]][2]
as.numeric(strsplit(XX[1,1],"#")[[1]][2])
q()
library(EFDR)
Z <- test.image()
Z <- test_image()
image(Z)
test_image
image(Z$z)
Z <- Z$z
Z <- test_image()
names(Z)
Z$grid
df <- Z$grid
df$z <- c(Z$z)
Z$z
c(Z$z)
class(c(Z$z))
df$z <- c(Z$z)
ggplot() + geom_tile(data=df,aes(x,y,fill=z))
library(ggplot2)
ggplot() + geom_tile(data=df,aes(x,y,fill=z))
class(df)
df$z <- c(Z$z)
head(df)
df <- Z$grid
c(Z$z)
df$z <- c(Z$z)
df <- Z$grid
head(df)
class(df)
df <- data.frame(Z$grid)
head(df)
df$z <- c(Z$z)
head(df)
ggplot() + geom_tile(data=df,aes(x,y,fill=z))
ggplot() + geom_tile(data=df,aes(Var1,Var2,fill=z))
df$z <- df$z + Var1.30
df$z <- df$z + df$Var1/30
ggplot() + geom_tile(data=df,aes(Var1,Var2,fill=z))
Z2 <- regrid(df = df,n1 = 64,n2 = 64,method = 'median_polish')
names(df) <- c("x","y","z")
Z2 <- regrid(df = df,n1 = 64,n2 = 64,method = 'median_polish')
class(Z2)
ggplot() + geom_tile(data=Z2,aes(x,y,fill=z))
regrid
n1=64
Z2 <- regrid(df = df,n1 = 64,n2 = 64,method = 'median_polish')
nrow(df)
df[runif(30,min = 1,max = 4096),] <- NULL
df <- df[-runif(30,min = 1,max = 4096),]
Z2 <- regrid(df = df,n1 = 64,n2 = 64,method = 'median_polish')
ggplot() + geom_tile(data=df,aes(x,y,fill=z))
ggplot() + geom_tile(data=Z2,aes(x,y,fill=z))
devtools::install_github("hafen/housingData")
install.packages("datadr")
devtools::install_github("tesseradata/datadr")
devtools::install_github("tesseradata/trelliscope")
library(housingData)
library(datadr); library(trelliscope)
# look at housing data
head(housing)
# divide by county and state
byCounty <- divide(housing,
by = c("county", "state"), update = TRUE)
head(byCounty)
byCountry_d <- group_by(housing,county,state)
library(dplyr)
byCountry_d <- group_by(housing,county,state)
priceQ <- drQuantile(byCounty,
var = "medListPriceSqft")
priceQ
xyplot(q ~ fval, data = priceQ,
scales = list(y = list(log = 10)))
head(priceQ)
xlim=c(min(df$x),max(df$x))
priceQ <- drQuantile(byCounty,
var = "medListPriceSqft")
priceQ
head(priceQ)
summarise(byCountry_d,median)
head(housing)
summarise(byCountry_d,median(medListPriceSqft,na.rm=T))
view()
# look at housing data
head(housing)
# divide by county and state
byCounty <- divide(housing,
by = c("county", "state"), update = TRUE)
# look at summaries
summary(byCounty)
# look at overall distribution of median list price
priceQ <- drQuantile(byCounty,
var = "medListPriceSqft")
xyplot(q ~ fval, data = priceQ,
scales = list(y = list(log = 10)))
# slope of fitted line of list price for each county
lmCoef <- function(x)
coef(lm(medListPriceSqft ~ time, data = x))[2]
# apply lmCoef to each subset
byCountySlope <- addTransform(byCounty, lmCoef)
# look at a subset of transformed data
byCountySlope[[1]]
# recombine all slopes into a single data frame
countySlopes <- recombine(byCountySlope, combRbind)
plot(sort(countySlopes$val))
# make a time series trelliscope display
vdbConn("housingjunk/vdb", autoYes = TRUE)
# make and test panel function
timePanel <- function(x)
xyplot(medListPriceSqft + medSoldPriceSqft ~ time,
data = x, auto.key = TRUE, ylab = "$ / Sq. Ft.")
timePanel(byCounty[[1]][[2]])
# make and test cognostics function
priceCog <- function(x) { list(
slope = cog(lmCoef(x), desc = "list price slope"),
meanList = cogMean(x$medListPriceSqft),
listRange = cogRange(x$medListPriceSqft),
nObs = cog(sum(!is.na(x$medListPriceSqft)),
desc = "number of non-NA list prices")
)}
priceCog(byCounty[[1]][[2]])
makeDisplay(byCounty,
name = "list_sold_vs_time",
desc = "List and sold price over time",
panelFn = timePanel, cogFn = priceCog,
width = 400, height = 400,
lims = list(x = "same"))
view()
view()
# install package with housing data
devtools::install_github("hafen/housingData")
library(housingData)
library(datadr); library(trelliscope)
# look at housing data
head(housing)
# divide by county and state
byCounty <- divide(housing,
by = c("county", "state"), update = TRUE)
# look at summaries
summary(byCounty)
# look at overall distribution of median list price
priceQ <- drQuantile(byCounty,
var = "medListPriceSqft")
xyplot(q ~ fval, data = priceQ,
scales = list(y = list(log = 10)))
# slope of fitted line of list price for each county
lmCoef <- function(x)
coef(lm(medListPriceSqft ~ time, data = x))[2]
# apply lmCoef to each subset
byCountySlope <- addTransform(byCounty, lmCoef)
# look at a subset of transformed data
byCountySlope[[1]]
# recombine all slopes into a single data frame
countySlopes <- recombine(byCountySlope, combRbind)
plot(sort(countySlopes$val))
# make a time series trelliscope display
vdbConn("housingjunk/vdb", autoYes = TRUE)
# make and test panel function
timePanel <- function(x)
xyplot(medListPriceSqft + medSoldPriceSqft ~ time,
data = x, auto.key = TRUE, ylab = "$ / Sq. Ft.")
timePanel(byCounty[[1]][[2]])
# make and test cognostics function
priceCog <- function(x) { list(
slope = cog(lmCoef(x), desc = "list price slope"),
meanList = cogMean(x$medListPriceSqft),
listRange = cogRange(x$medListPriceSqft),
nObs = cog(sum(!is.na(x$medListPriceSqft)),
desc = "number of non-NA list prices")
)}
priceCog(byCounty[[1]][[2]])
view()
view()
q()
t <- 1:10
t <- seq(1,10,length=100)
t
x <- sin(t)
plot(t,x)
D <- dist(t)
D
Sigma <- -D^2
Sigma
D <- dist(t,upper = T)
Sigma
D
Sigma <- -D^2
image(Sigma)
Sigma
S <- as.matrix(Sigma)
image(Sigma)
image(S)
y <- solve(Sigma*x)
dim(Sigma)
y <- solve(S)%*%x
solve(S)
S
MVST::Matern
MVST::Matern(t)
MVST::Matern(as.matrix(D))
image(D)
S <- MVST::Matern(as.matrix(D))
image(S)
solve(S)
y <- solve(S)%*%x
plot(y)
plot(y-[1:10])
y[1]
solve(S)
solve(S)[1,]
plot(solve(S)[1,])
plot(solve(S)[1,])%*% x
t(solve(S)[1,])%*% x
y[1]
plot(S[1,])
plot(1/S[1,])
library(EFDR)
help(regrid)
library(RCurl())
sst <- getURL("http://www.stat.missouri.edu/~wikle/SSTlonlat.dat") %>%
textConnection() %>%
read.table(col.names = c("x","y")) %>%
cbind(getURL("http://www.stat.missouri.edu/~wikle/SST011970_032003.dat") %>%
textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table())[,1])
library(dplyr)
sst <- getURL("http://www.stat.missouri.edu/~wikle/SSTlonlat.dat") %>%
textConnection() %>%
read.table(col.names = c("x","y")) %>%
cbind(getURL("http://www.stat.missouri.edu/~wikle/SST011970_032003.dat") %>%
textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table())[,1])
head(sst)
head( gather(sst,date,z,-x,-y,-land) )
library(tidyr)
head( gather(sst,date,z,-x,-y,-land) )
i=5
floor((i-1)/12)+1970)
floor((i-1)/12)+1970
i/12
i=53
floor((i-1)/12)+1970
(i-1)/12
i
(i-1)%%12
i=1
(i-1)%%12
i%%12
i=53
i%%12
i=12
i%%12
(i-1)%%12+1
(i-1)%%12+1
i=2
(i-1)%%12+1
i=13
(i-1)%%12+1
month(1)
paste(c("a","c"))
paste0(c("a","c"))
cat(c("a","c"))
cat("a","c")
x <- cat("a","c")
x
cat(paste0(c("a","c")))
cat(paste0("a","c"))
paste0("a","c")
paste("a","c")
aste(months[(i-1)%%12+1],
floor((i-1)/12)+1970))
paste(months[(i-1)%%12+1],
floor((i-1)/12)+1970))
paste(months[(i-1)%%12+1], floor((i-1)/12)+1970)
months[(i-1)%%12+1]
i
months
help(months)
months(1)
mon <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")
paste(mon[(i-1)%%12+1],
floor((i-1)/12)+1970)
library(doParallel)
detectCores
detectCores()
library(dplyr)
library(tidyr)
library(ggplot2)
library(foreach)
library(parallel)
library(doParallel)
library(RCurl)
sst <- getURL("http://www.stat.missouri.edu/~wikle/SSTlonlat.dat") %>%
textConnection() %>%
read.table(col.names = c("x","y")) %>%
cbind(getURL("http://www.stat.missouri.edu/~wikle/SST011970_032003.dat") %>%
textConnection() %>%
read.table()) %>%
mutate(land = (getURL("http://www.stat.missouri.edu/~wikle/SSTlandmask.dat") %>%
textConnection() %>%
read.table())[,1]) %>%
gather(date,z,-x,-y,-land) %>%
mutate(time = as.numeric(date)) %>%
select(-date) %>%
subset(x > 155 & x < 280 & y <14&land==0)
n1 =        64
n2 =        32
idp =       0.5
nmax =      6
alpha =     0.05        # 5% significant level
wf =        "la8"       # wavelet name
J =         3           # 3 resolutions
b =         5
parallel =  detectCores()/2
cl <- makeCluster(parallel)
registerDoParallel(cl)
X <- foreach (t = unique(sst$time),.combine=rbind) %dopar% {
library(EFDR)
sst_t <- subset(sst,time==t)
sst_t_regrid <- regrid(sst_t,n1=n1,n2=n2,idp= idp,nmax = nmax)
sst_t_regrid$zsig <- c(test.efdr(df.to.mat(sst_t_regrid), wf=wf,J=J, alpha = alpha,parallel=parallel)$Z)
sst_t_regrid$t <- t
sst_t_regrid
}
stopCluster()
stopCluster(cl)
stopCluster(cl)
stopCluster(cl)
q()
X <- matrix(c(1,2,3,4,5,6),2,3)
image(X)
X
q()
q()
library(devtools)
setwd("/media/andrew/Windows/Users/azm/Dropbox/CurrentProjects/Wollongong/pkgs/EFDR/vignettes2")
document("..")
release("..")
install("../EFDR_1.0.tar.gz",repos=NULL,type="source")
install.packages("../EFDR_1.0.tar.gz",repos=NULL,type="source")
library(EFDR)
vignettes()
vignette()
library(EFDR)
vignette(EFDR)
vignette("EFDR")
vignette()
install.packages("../EFDR_1.0.tar.gz",repos=NULL,type="source")
library(EFDR)
vignette()
q()
